{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행동 스티커 만들기\n",
    "\n",
    "## 평가 루브릭\n",
    "\n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "평가문항\t상세기준\n",
    "### 1. tfrecord를 활용한 데이터셋 구성과 전처리를 통해 프로젝트 베이스라인 구성을 확인하였다.\n",
    "    - MPII 데이터셋을 기반으로 1epoch에 30분 이내에 학습가능한 베이스라인을 구축하였다.\n",
    "### 2. simplebaseline 모델을 정상적으로 구현하였다.\n",
    "    - simplebaseline 모델을 구현하여 실습코드의 모델을 대체하여 정상적으로 학습이 진행되었다.\n",
    "### 3. Hourglass 모델과 simplebaseline 모델을 비교분석한 결과를 체계적으로 정리하였다.\n",
    "    - 두 모델의 pose estimation 테스트결과 이미지 및 학습진행상황 등을 체계적으로 비교분석하였다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_sb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from simplebaseline import Simplebaseline\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        # \"we use rmsprop with a learning rate of 2.5e-4.\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        images = tf.cast(labels, dtype='float32')\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        images = tf.cast(labels, dtype='float32')\n",
    "        labels = tf.cast(labels, dtype='float32')\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "        summary_writer.set_as_default()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "            self.lr_decay()\n",
    "            tf.summary.scalar('epoch learning rate',\n",
    "                              self.current_learning_rate)\n",
    "\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "            self.version, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = Simplebaseline(IMAGE_SHAPE, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tfrecords_dir = './tfrecords_mpii/'\n",
    "    train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "    val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "    epochs = 5\n",
    "    batch_size = 8\n",
    "    num_heatmap = 16\n",
    "    tensorboard_dir = './logs/'\n",
    "    learning_rate = 0.0007\n",
    "    start_epoch = 1\n",
    "\n",
    "    automatic_gpu_usage()\n",
    "\n",
    "    pretrained_path = None # './models/model-v0.0.3-epoch-1-loss-1.0744.h5'\n",
    "\n",
    "    train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplebaseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential_9 (Sequential)    (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "def Simplebaseline(input_shape=(256, 256, 3), num_heatmap=16):\n",
    "    resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "    upconv = _make_deconv_layer(3)\n",
    "    final_layer = tf.keras.layers.Conv2D(num_heatmap, kernel_size=(1,1), padding='same')\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)    \n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "m = Simplebaseline()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: 모델 바꿔보기\n",
    "\n",
    "## STEP 1 : simplebaseline 모델 완성하기\n",
    "- simplebaseline.py 프로그램 완성\n",
    "\n",
    "## STEP 2 : simplebaseline 모델로 변경하여 훈련하기\n",
    "- train_sb.py 218라인의 모델 선언 부분을 simplebaseline 모델로 변경하였으나, 학습이 잘 되지 않았음\n",
    "\n",
    "## STEP 3 : 두 모델의 비교\n",
    "- 실습에서 다룬 StackedHourglass Network와 Simplebaseline 모델을 둘 다 동일한 Epoch 수만큼 학습하여 그 결과를 비교해 봅니다.\n",
    "\n",
    "## Pose Estimation 결과 시각화 (정성적 비교)\n",
    "\n",
    "## 학습 진행 경과\n",
    "\n",
    "### - 첫번째 시도에서 StackedHourglass Network 모델은 Validation Loss가 15 Epoch에서 1.5659 까지 낮아졌다. \n",
    "\n",
    "![title](./results/learning_result1.png)\n",
    "\n",
    "### - 두번째 시도에서는 위의 학습 결과를 불러와서 Validation Loss가 2 Epoch에서 1.2269 까지 낮아졌다. \n",
    "\n",
    "![title](./results/learning_result.png)\n",
    "\n",
    "\n",
    "## 결과 시각화\n",
    "\n",
    "### Test 1 결과 이미지\n",
    "\n",
    "![title](./results/test0_p.jpg)\n",
    "![title](./results/test0_s.jpg)\n",
    "\n",
    "### Test 2 결과 이미지\n",
    "\n",
    "![title](./results/test1_p.jpg)\n",
    "![title](./results/test1_s.jpg)\n",
    "\n",
    "### Test 3 결과 이미지\n",
    "\n",
    "![title](./results/test2_p.jpg)\n",
    "![title](./results/test2_s.jpg)\n",
    "\n",
    "### Test 4 결과 이미지\n",
    "\n",
    "![title](./results/test3_p.jpg)\n",
    "![title](./results/test3_s.jpg)\n",
    "\n",
    "### Test 5 결과 이미지\n",
    "\n",
    "![title](./results/test4_p.jpg)\n",
    "![title](./results/test4_s.jpg)\n",
    "\n",
    "### Test 6 결과 이미지\n",
    "\n",
    "![title](./results/test5_p.jpg)\n",
    "![title](./results/test5_s.jpg)\n",
    "\n",
    "### Test 7 결과 이미지\n",
    "\n",
    "![title](./results/test6_p.jpg)\n",
    "![title](./results/test6_s.jpg)\n",
    "\n",
    "\n",
    "### Test 8 결과 이미지\n",
    "\n",
    "![title](./results/test7_p.jpg)\n",
    "![title](./results/test7_s.jpg)\n",
    "\n",
    "\n",
    "### Test 9 결과 이미지\n",
    "\n",
    "![title](./results/test8_p.jpg)\n",
    "![title](./results/test8_s.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
